# Quick Start Guide - Interview Demo

## üéØ 5-Minute Setup

```bash
# 1. Activate venv (already created)
source .venv/bin/activate

# 2. Start server
python manage.py runserver 8001

# Open another terminal for API testing ‚Üì
```

## üöÄ Live Demo Commands

### Show Pipeline Structure
```bash
# See full pipeline with stages/substages
curl -s http://127.0.0.1:8001/api/pipelines/1/ | python -m json.tool
```

### Show Run Summary
```bash
# Get complete run breakdown with all results
curl -s http://127.0.0.1:8001/api/runs/2/summary/ | python -m json.tool
```

### Show Trend Analysis
```bash
# Last 5 runs with scores
curl -s 'http://127.0.0.1:8001/api/pipelines/1/trend/?n=5' | python -m json.tool
```

### Create New Run
```bash
curl -X POST http://127.0.0.1:8001/api/runs/ \
  -H "Content-Type: application/json" \
  -d '{"pipeline": 1, "triggered_by": "Interview Demo"}'
```

### Update Substage (simulate progress)
```bash
# Update Source Sync to 100%
curl -X POST http://127.0.0.1:8001/api/runs/1/update_stage/ \
  -H "Content-Type: application/json" \
  -d '{"substage_id": 1, "completion_percent": 100, "status": "completed"}'

# Returns updated overall_score
```

## üìä What Each Endpoint Shows

| Endpoint | What It Demonstrates |
|----------|---------------------|
| `/api/pipelines/1/` | JSON-driven config, nested hierarchy |
| `/api/runs/2/summary/` | Real-time score calculation, full breakdown |
| `/api/pipelines/1/trend/` | Historical analysis, metrics over time |
| `POST /api/runs/` | Auto-creation of results via signals |
| `POST /api/runs/{id}/update_stage/` | Dynamic score recalculation |

## üé§ Interview Talking Points

### 1. System Design (show ARCHITECTURE.md)
- "I designed a hierarchical model: Organization ‚Üí Team ‚Üí Project ‚Üí Pipeline"
- "This mirrors real org structure at TI - multiple teams collaborating"
- "Each team owns specific stages (Build team, Test team, Deploy team)"

### 2. JSON Configuration (show configs/pipeline_sysfw.json)
- "Pipelines are fully configurable via JSON - no code changes needed"
- "Each stage/substage has weight, Definition of Done, manual/auto validation"
- "Teams can add new pipelines by creating JSON and running one command"

### 3. Weighted Scoring (show /api/runs/2/summary/)
- "Each stage has weight (Build: 40%, Test: 40%, Deploy: 20%)"
- "Score = Œ£(stage.weight √ó completion%) normalized by total weight"
- "This gives accurate progress tracking for complex workflows"

### 4. API Design (show curl commands)
- "RESTful design with custom actions (trend, summary, update_stage)"
- "Nested serializers provide full context in one request"
- "POST /update_stage triggers automatic score recalculation"

### 5. Automation (explain signals.py)
- "When a Run is created, signals auto-create all StageResult/SubStageResult rows"
- "This ensures every run has a complete structure ready for updates"
- "Reduces boilerplate and ensures data consistency"

### 6. Scalability
- "Current setup handles unlimited pipelines/teams/projects"
- "JSON configs allow self-service for teams"
- "Can easily add webhook integration for Jenkins/GitHub Actions"

## üîç Code Walkthrough Order

1. **models.py** (lines 1-130)
   - Show Organization ‚Üí Team ‚Üí Project hierarchy
   - Explain Pipeline ‚Üí Stage ‚Üí SubStage with weights
   - Point out Run.overall_score property

2. **serializers.py** (lines 1-80)
   - Show nested serializers (StageResultSerializer includes SubStageResultSerializer)
   - Explain how RunSerializer computes overall_score

3. **views.py** (lines 1-70)
   - Show custom actions: `@action(detail=True, methods=['get'])` for trend
   - Explain update_stage endpoint and score recalculation

4. **signals.py** (lines 1-20)
   - Show post_save signal that auto-creates results
   - Explain why this ensures data consistency

5. **services.py** (lines 1-40)
   - Show calculate_run_score algorithm
   - Explain normalization by total stage weight

6. **management/commands/load_pipeline_config.py**
   - Show how JSON ‚Üí DB transformation works
   - Explain update_or_create for idempotency

## üé¨ Demo Script (2-3 minutes)

```
1. "Let me show you the tracker I built for Apple interview"

2. [Terminal 1] python manage.py runserver 8001

3. "First, here's the pipeline structure loaded from JSON"
   [Terminal 2] curl http://127.0.0.1:8001/api/pipelines/1/ | python -m json.tool
   ‚Üí Point out stages, substages, weights

4. "Now let's see a completed run with full breakdown"
   curl http://127.0.0.1:8001/api/runs/2/summary/ | python -m json.tool
   ‚Üí Point out overall_score: 100.0, all substages completed

5. "Here's the trend showing score progression over time"
   curl 'http://127.0.0.1:8001/api/pipelines/1/trend/?n=5' | python -m json.tool
   ‚Üí Show scores: 100%, 58%, 82% across runs

6. "Let me create a new run and update it in real-time"
   curl -X POST http://127.0.0.1:8001/api/runs/ \
     -H "Content-Type: application/json" \
     -d '{"pipeline": 1, "triggered_by": "Live Demo"}'
   ‚Üí Note overall_score: 0.0 initially

7. "Now I'll mark first substage complete"
   curl -X POST http://127.0.0.1:8001/api/runs/6/update_stage/ \
     -H "Content-Type: application/json" \
     -d '{"substage_id": 1, "completion_percent": 100}'
   ‚Üí Show overall_score increased

8. "The system automatically recalculated: Build stage is 1/3 complete,
    which is 33% √ó 40% weight = 13% overall"

9. [Optional] "I can show you the code - signals auto-create results,
    services calculate scores, serializers nest the hierarchy"
```

## üìù Questions You Might Get

**Q: How would you handle authentication?**
A: Add JWT tokens via `djangorestframework-simplejwt`, team-based permissions via Django groups

**Q: How does this scale to 1000s of runs?**
A: Add database indexes on run.start_time, run.pipeline_id, pagination (already implemented), async tasks for score calculation

**Q: How would teams integrate this with Jenkins?**
A: Jenkins webhook ‚Üí POST /api/runs/, Jenkins stages ‚Üí POST /api/runs/{id}/update_stage/

**Q: Why Django over FastAPI/Flask?**
A: Django admin for quick browsing, ORM for complex relationships, DRF for consistent API patterns, signals for automation

**Q: How would you add a dashboard?**
A: React frontend consuming these APIs, Chart.js for trend graphs, real-time updates via WebSockets

## ‚úÖ Key Achievements to Highlight

- ‚úÖ **Full-stack thinking**: Models, API, automation, deployment considerations
- ‚úÖ **Production-quality**: Error handling, constraints, normalization, signals
- ‚úÖ **Real-world problem**: Solves actual TI SYSFW build tracking
- ‚úÖ **Scalable design**: JSON configs, multi-team, unlimited pipelines
- ‚úÖ **Clean code**: PEP8, type hints, docstrings, separation of concerns
- ‚úÖ **Well-documented**: README, ARCHITECTURE.md, INTERVIEW_SUMMARY.md
- ‚úÖ **Demo-ready**: Working server, sample data, curl commands

## üö® If Something Goes Wrong

```bash
# Server not starting?
lsof -ti:8001 | xargs kill -9

# Database issues?
rm db.sqlite3
python manage.py migrate
python manage.py migrate tracker

# Need fresh data?
python manage.py shell -c "
from tracker.models import *
Organization.objects.all().delete()
# Then re-run seed commands from README
"
```

---

**Pro tip**: Open this file, ARCHITECTURE.md, and README.md in VS Code tabs during interview for quick reference. Practice the demo script 2-3 times to be smooth.

Good luck! üöÄ
